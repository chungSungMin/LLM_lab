{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "782d1026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "import time \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a91d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if device == \"cpu\":\n",
    "    print(\"해당 작업이 CPU에서 진행됩니다.....🙊\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d19db88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_usage():\n",
    "    # MB 단위로 출력한다.\n",
    "    return torch.cuda.memory_allocated() / 1024 ** 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9238a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meauser_inference_speed(model, tokenizer, text, num_runs=100):\n",
    "    model.eval()\n",
    "    times = []\n",
    "\n",
    "    # 워밍업을 위한 단계 ( 캐쉬, 로드 등 다양한 변수를 제외하고 측정하기 위함 )\n",
    "    for _ in range(num_runs):\n",
    "        # return_tensors=\"pt\"는 PyTorch를 위한 데이터를 반환해준다.\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        _  = model(**inputs)\n",
    "\n",
    "    for _ in range(num_runs):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        # time.perf_counter()가 성능 검증할떄 time.time()보다 세밀하다.\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "        end_time = time.perf_counter()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "    avg_time = np.mean(times) * 1000\n",
    "    return avg_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ba4920c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------🦇FP32 모델 로드 및 평가 진행🦇-----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 🐗 FP32 모델 GPU 메모리 사용량: 610.49658203125 MB\n",
      " 🐗 FP32 모델 평균 추론 시간 : 5.0102535472251475(second)\n"
     ]
    }
   ],
   "source": [
    "print(\"------🦇FP32 모델 로드 및 평가 진행🦇-----\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "fp32_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "fp32_memory = get_gpu_memory_usage()\n",
    "\n",
    "sample_text = \"This is a fantastic moive, I really enjoyed it!!\"\n",
    "fp32_latency = meauser_inference_speed(fp32_model, tokenizer, sample_text)\n",
    "\n",
    "print(f\" 🐗 FP32 모델 GPU 메모리 사용량: {fp32_memory} MB\")\n",
    "print(f\" 🐗 FP32 모델 평균 추론 시간 : {fp32_latency}(second)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "963dfc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확한 측정을 위해서 fp32 모델을 제거 합니다.\n",
    "del fp32_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72225b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /venv/main/lib/python3.12/site-packages (0.47.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /venv/main/lib/python3.12/site-packages (from bitsandbytes) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.12/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7cdbfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----🦇INT8 모델 로드 및 평가🦇-----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 🐗 INT8 모델 GPU 메모리 사용량: 375.6787109375 MB\n",
      " 🐗 INT8 모델 평균 추론 시간 : 12.692619708832353(second)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "print(\"----🦇INT8 모델 로드 및 평가🦇-----\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", # Normal Float4\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # 4비트로 저장하지만 계산시 16비트로 변환\n",
    ")\n",
    "\n",
    "int8_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "int8_memory = get_gpu_memory_usage()\n",
    "int8_latency = meauser_inference_speed(int8_model, tokenizer, sample_text)\n",
    "\n",
    "print(f\" 🐗 INT8 모델 GPU 메모리 사용량: {int8_memory} MB\")\n",
    "print(f\" 🐗 INT8 모델 평균 추론 시간 : {int8_latency}(second)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46a2864a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====🦟최종 정리🦟=====\n",
      "모델 유형 | GPU 메모리 사용량 | 평균 추론 시간 \n",
      "FP32 | 610.49658203125 | 5.0102535472251475\n",
      "INT8 | 375.6787109375 (대략 1.0% 절약) | 12.692619708832353\n"
     ]
    }
   ],
   "source": [
    "print(f\"=====🦟최종 정리🦟=====\")\n",
    "\n",
    "print(f\"모델 유형 | GPU 메모리 사용량 | 평균 추론 시간 \")\n",
    "print(f\"FP32 | {fp32_memory} | {fp32_latency}\")\n",
    "print(f\"INT8 | {int8_memory} (대략 {fp32_memory // int8_memory}% 절약) | {int8_latency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c198f1",
   "metadata": {},
   "source": [
    "### 결론\n",
    "Quantization을 사용하면 확실히 GPU RAM을 줄입니다. 하지만 추론 속도의 경우 양자화 및 역양자화 과정 등 오버헤드가 발생하며 오히려 연산 시간이 더 오래걸리는 것을 확인할 수 있었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a4b55b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

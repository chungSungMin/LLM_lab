{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "782d1026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "import time \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a91d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if device == \"cpu\":\n",
    "    print(\"í•´ë‹¹ ì‘ì—…ì´ CPUì—ì„œ ì§„í–‰ë©ë‹ˆë‹¤.....ğŸ™Š\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d19db88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_usage():\n",
    "    # MB ë‹¨ìœ„ë¡œ ì¶œë ¥í•œë‹¤.\n",
    "    return torch.cuda.memory_allocated() / 1024 ** 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9238a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meauser_inference_speed(model, tokenizer, text, num_runs=100):\n",
    "    model.eval()\n",
    "    times = []\n",
    "\n",
    "    # ì›Œë°ì—…ì„ ìœ„í•œ ë‹¨ê³„ ( ìºì‰¬, ë¡œë“œ ë“± ë‹¤ì–‘í•œ ë³€ìˆ˜ë¥¼ ì œì™¸í•˜ê³  ì¸¡ì •í•˜ê¸° ìœ„í•¨ )\n",
    "    for _ in range(num_runs):\n",
    "        # return_tensors=\"pt\"ëŠ” PyTorchë¥¼ ìœ„í•œ ë°ì´í„°ë¥¼ ë°˜í™˜í•´ì¤€ë‹¤.\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        _  = model(**inputs)\n",
    "\n",
    "    for _ in range(num_runs):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        # time.perf_counter()ê°€ ì„±ëŠ¥ ê²€ì¦í• ë–„ time.time()ë³´ë‹¤ ì„¸ë°€í•˜ë‹¤.\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "        end_time = time.perf_counter()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "    avg_time = np.mean(times) * 1000\n",
    "    return avg_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ba4920c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ğŸ¦‡FP32 ëª¨ë¸ ë¡œë“œ ë° í‰ê°€ ì§„í–‰ğŸ¦‡-----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ğŸ— FP32 ëª¨ë¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 610.49658203125 MB\n",
      " ğŸ— FP32 ëª¨ë¸ í‰ê·  ì¶”ë¡  ì‹œê°„ : 5.0102535472251475(second)\n"
     ]
    }
   ],
   "source": [
    "print(\"------ğŸ¦‡FP32 ëª¨ë¸ ë¡œë“œ ë° í‰ê°€ ì§„í–‰ğŸ¦‡-----\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "fp32_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "fp32_memory = get_gpu_memory_usage()\n",
    "\n",
    "sample_text = \"This is a fantastic moive, I really enjoyed it!!\"\n",
    "fp32_latency = meauser_inference_speed(fp32_model, tokenizer, sample_text)\n",
    "\n",
    "print(f\" ğŸ— FP32 ëª¨ë¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {fp32_memory} MB\")\n",
    "print(f\" ğŸ— FP32 ëª¨ë¸ í‰ê·  ì¶”ë¡  ì‹œê°„ : {fp32_latency}(second)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "963dfc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •í™•í•œ ì¸¡ì •ì„ ìœ„í•´ì„œ fp32 ëª¨ë¸ì„ ì œê±° í•©ë‹ˆë‹¤.\n",
    "del fp32_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72225b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /venv/main/lib/python3.12/site-packages (0.47.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /venv/main/lib/python3.12/site-packages (from bitsandbytes) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.12/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7cdbfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ğŸ¦‡INT8 ëª¨ë¸ ë¡œë“œ ë° í‰ê°€ğŸ¦‡-----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ğŸ— INT8 ëª¨ë¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 375.6787109375 MB\n",
      " ğŸ— INT8 ëª¨ë¸ í‰ê·  ì¶”ë¡  ì‹œê°„ : 12.692619708832353(second)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "print(\"----ğŸ¦‡INT8 ëª¨ë¸ ë¡œë“œ ë° í‰ê°€ğŸ¦‡-----\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", # Normal Float4\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # 4ë¹„íŠ¸ë¡œ ì €ì¥í•˜ì§€ë§Œ ê³„ì‚°ì‹œ 16ë¹„íŠ¸ë¡œ ë³€í™˜\n",
    ")\n",
    "\n",
    "int8_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "int8_memory = get_gpu_memory_usage()\n",
    "int8_latency = meauser_inference_speed(int8_model, tokenizer, sample_text)\n",
    "\n",
    "print(f\" ğŸ— INT8 ëª¨ë¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {int8_memory} MB\")\n",
    "print(f\" ğŸ— INT8 ëª¨ë¸ í‰ê·  ì¶”ë¡  ì‹œê°„ : {int8_latency}(second)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46a2864a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====ğŸ¦Ÿìµœì¢… ì •ë¦¬ğŸ¦Ÿ=====\n",
      "ëª¨ë¸ ìœ í˜• | GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | í‰ê·  ì¶”ë¡  ì‹œê°„ \n",
      "FP32 | 610.49658203125 | 5.0102535472251475\n",
      "INT8 | 375.6787109375 (ëŒ€ëµ 1.0% ì ˆì•½) | 12.692619708832353\n"
     ]
    }
   ],
   "source": [
    "print(f\"=====ğŸ¦Ÿìµœì¢… ì •ë¦¬ğŸ¦Ÿ=====\")\n",
    "\n",
    "print(f\"ëª¨ë¸ ìœ í˜• | GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | í‰ê·  ì¶”ë¡  ì‹œê°„ \")\n",
    "print(f\"FP32 | {fp32_memory} | {fp32_latency}\")\n",
    "print(f\"INT8 | {int8_memory} (ëŒ€ëµ {fp32_memory // int8_memory}% ì ˆì•½) | {int8_latency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c198f1",
   "metadata": {},
   "source": [
    "### ê²°ë¡ \n",
    "Quantizationì„ ì‚¬ìš©í•˜ë©´ í™•ì‹¤íˆ GPU RAMì„ ì¤„ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì¶”ë¡  ì†ë„ì˜ ê²½ìš° ì–‘ìí™” ë° ì—­ì–‘ìí™” ê³¼ì • ë“± ì˜¤ë²„í—¤ë“œê°€ ë°œìƒí•˜ë©° ì˜¤íˆë ¤ ì—°ì‚° ì‹œê°„ì´ ë” ì˜¤ë˜ê±¸ë¦¬ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a4b55b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
